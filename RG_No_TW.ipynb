{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtRLaJN/5xvDqGWL360b/5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/intanelaqsha/Grievances-Event/blob/main/RG_No_TW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NO TIME WINDOW**"
      ],
      "metadata": {
        "id": "ar3GBGHe-csD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whats new?\n",
        "\n",
        "1. No time window"
      ],
      "metadata": {
        "id": "8MvTv7DdBs7M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WjZsz_OsRcK",
        "outputId": "cda2f76f-a4e1-4002-ee4e-35dcdf3e07a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[STEP 1] Load & Normalize...\n",
            "✓ Total expanded rows: 3488\n",
            "\n",
            "[STEP 2] Clustering by Source...\n",
            "✓ Total events after Step 2: 1376\n",
            "\n",
            "[STEP 3] Final MHID merge...\n",
            "\n",
            "[STEP 4] Adding Plot & Mill Groups...\n",
            "✓ Added Plot & Mill group columns\n",
            "\n",
            "[STEP 5] Adding company tracker columns...\n",
            "✓ Added company tracker columns\n",
            "\n",
            "✅ FINAL DONE\n",
            "Final MHID count: 667\n",
            "Output saved to: Final_Merged_Notw.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# =====================================================\n",
        "# CONFIG\n",
        "# =====================================================\n",
        "INPUT_FILE = \"Grievances-Grid view 3.csv\"\n",
        "FINAL_OUT = \"Final_Merged_Notw.csv\"\n",
        "\n",
        "# =====================================================\n",
        "# HELPERS\n",
        "# =====================================================\n",
        "def split_list(cell):\n",
        "    if pd.isna(cell) or str(cell).strip() == \"\":\n",
        "        return []\n",
        "    s = str(cell).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "    parts = [p.strip() for p in re.split(\"[,;]\", s)]\n",
        "    return list({p for p in parts if p})\n",
        "\n",
        "def split_source(val):\n",
        "    if pd.isna(val) or str(val).strip() == \"\":\n",
        "        return []\n",
        "    s = str(val).strip()\n",
        "    parts = re.split(r\",(?=\\S)\", s)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "def to_list(cell):\n",
        "    if pd.isna(cell) or str(cell).strip() == \"\":\n",
        "        return []\n",
        "    return [x.strip() for x in str(cell).split(\",\") if x.strip()]\n",
        "\n",
        "def uniq_list(x):\n",
        "    return sorted(list(set(x)))\n",
        "\n",
        "# =====================================================\n",
        "# STEP 1 – LOAD + NORMALIZE + EXPAND SOURCE\n",
        "# =====================================================\n",
        "print(\"\\n[STEP 1] Load & Normalize...\")\n",
        "\n",
        "df = pd.read_csv(INPUT_FILE, dtype=str)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "df[\"Raw_ID\"] = df.index.astype(int)\n",
        "\n",
        "multi_cols = [\"Suppliers\", \"Mills\", \"PIOConcessions\", \"Issues\"]\n",
        "\n",
        "for col in multi_cols:\n",
        "    df[col] = df[col].apply(split_list)\n",
        "\n",
        "df[\"Source\"] = df[\"Source\"].apply(split_source)\n",
        "\n",
        "expanded_rows = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    sources = row[\"Source\"]\n",
        "\n",
        "    if len(sources) == 0:\n",
        "        new_row = row.copy()\n",
        "        new_row[\"Source\"] = None\n",
        "        expanded_rows.append(new_row)\n",
        "    else:\n",
        "        for s in sources:\n",
        "            new_row = row.copy()\n",
        "            new_row[\"Source\"] = s\n",
        "            expanded_rows.append(new_row)\n",
        "\n",
        "df_expanded = pd.DataFrame(expanded_rows)\n",
        "df_expanded[\"Row_ID\"] = df_expanded.index.astype(int)\n",
        "\n",
        "print(\"✓ Total expanded rows:\", len(df_expanded))\n",
        "\n",
        "# =====================================================\n",
        "# STEP 2 – MERGE PER SOURCE (same as your version)\n",
        "# =====================================================\n",
        "print(\"\\n[STEP 2] Clustering by Source...\")\n",
        "\n",
        "events = []\n",
        "event_id = 1\n",
        "\n",
        "for source, group in df_expanded.groupby(\"Source\"):\n",
        "    source_events = []\n",
        "\n",
        "    for _, row in group.iterrows():\n",
        "        sup = set(row[\"Suppliers\"])\n",
        "        mil = set(row[\"Mills\"])\n",
        "        pio = set(row[\"PIOConcessions\"])\n",
        "        iss = set(row[\"Issues\"])\n",
        "\n",
        "        gid = row[\"ID\"]\n",
        "        date_filed = row[\"Date Filed\"]\n",
        "\n",
        "        merged = False\n",
        "\n",
        "        for evt in source_events:\n",
        "\n",
        "            overlap = (\n",
        "                len(sup & set(evt[\"Suppliers\"])) > 0 or\n",
        "                len(mil & set(evt[\"Mills\"])) > 0 or\n",
        "                len(pio & set(evt[\"PIOConcessions\"])) > 0\n",
        "            )\n",
        "\n",
        "            if overlap:\n",
        "                evt[\"Suppliers\"] = uniq_list(evt[\"Suppliers\"] + list(sup))\n",
        "                evt[\"Mills\"] = uniq_list(evt[\"Mills\"] + list(mil))\n",
        "                evt[\"PIOConcessions\"] = uniq_list(evt[\"PIOConcessions\"] + list(pio))\n",
        "                evt[\"Issues\"] = uniq_list(evt[\"Issues\"] + list(iss))\n",
        "\n",
        "                evt[\"Grievance_List\"].append(gid)\n",
        "                evt[\"Grievance_List\"] = uniq_list(evt[\"Grievance_List\"])\n",
        "                evt[\"Grievance_Count\"] = len(evt[\"Grievance_List\"])\n",
        "\n",
        "                evt[\"Date_Filed_List\"].append(date_filed)\n",
        "                evt[\"Date_Filed_List\"] = uniq_list(evt[\"Date_Filed_List\"])\n",
        "                evt[\"Date_Filed\"] = min(evt[\"Date_Filed_List\"])\n",
        "\n",
        "                merged = True\n",
        "                break\n",
        "\n",
        "        if not merged:\n",
        "            source_events.append({\n",
        "                \"Event_ID\": f\"EVT_{event_id}\",\n",
        "                \"Source\": source,\n",
        "                \"Suppliers\": list(sup),\n",
        "                \"Mills\": list(mil),\n",
        "                \"PIOConcessions\": list(pio),\n",
        "                \"Issues\": list(iss),\n",
        "                \"Grievance_List\": [gid],\n",
        "                \"Grievance_Count\": 1,\n",
        "                \"Date_Filed_List\": [date_filed],\n",
        "                \"Date_Filed\": date_filed\n",
        "            })\n",
        "            event_id += 1\n",
        "\n",
        "    events.extend(source_events)\n",
        "\n",
        "df_step2 = pd.DataFrame(events)\n",
        "print(\"✓ Total events after Step 2:\", len(df_step2))\n",
        "\n",
        "# =====================================================\n",
        "# STEP 3 – FINAL MERGE (NO TIME WINDOW)\n",
        "# =====================================================\n",
        "print(\"\\n[STEP 3] Final MHID merge...\")\n",
        "\n",
        "for col in [\"Suppliers\",\"Mills\",\"PIOConcessions\",\"Issues\",\"Grievance_List\",\"Source\"]:\n",
        "    df_step2[col] = df_step2[col].apply(lambda x: x if isinstance(x, list) else to_list(x))\n",
        "\n",
        "merged_events = []\n",
        "mhid = 1\n",
        "\n",
        "for _, row in df_step2.iterrows():\n",
        "\n",
        "    merged = False\n",
        "\n",
        "    for evt in merged_events:\n",
        "\n",
        "        supplier_overlap = len(set(row[\"Suppliers\"]) & set(evt[\"Suppliers\"])) >= 1\n",
        "\n",
        "        row_has_infra = len(row[\"Mills\"]) > 0 or len(row[\"PIOConcessions\"]) > 0\n",
        "        evt_has_infra = len(evt[\"Mills\"]) > 0 or len(evt[\"PIOConcessions\"]) > 0\n",
        "\n",
        "        infra_overlap = (\n",
        "            len(set(row[\"Mills\"]) & set(evt[\"Mills\"])) >= 1 or\n",
        "            len(set(row[\"PIOConcessions\"]) & set(evt[\"PIOConcessions\"])) >= 1\n",
        "        )\n",
        "\n",
        "        # ✅ New logic: MUST overlap in supplier AND infra\n",
        "        if supplier_overlap and infra_overlap:\n",
        "\n",
        "            evt[\"Suppliers\"] = uniq_list(evt[\"Suppliers\"] + row[\"Suppliers\"])\n",
        "            evt[\"Mills\"] = uniq_list(evt[\"Mills\"] + row[\"Mills\"])\n",
        "            evt[\"PIOConcessions\"] = uniq_list(evt[\"PIOConcessions\"] + row[\"PIOConcessions\"])\n",
        "            evt[\"Issues\"] = uniq_list(evt[\"Issues\"] + row[\"Issues\"])\n",
        "            evt[\"Source\"] = uniq_list(evt[\"Source\"] + row[\"Source\"])\n",
        "            evt[\"Grievance_List\"] = uniq_list(evt[\"Grievance_List\"] + row[\"Grievance_List\"])\n",
        "\n",
        "            evt[\"Grievance_Count\"] = len(evt[\"Grievance_List\"])\n",
        "            merged = True\n",
        "            break\n",
        "\n",
        "    if not merged:\n",
        "        merged_events.append({\n",
        "            \"MHID\": f\"MHID_{mhid}\",\n",
        "            \"Suppliers\": row[\"Suppliers\"],\n",
        "            \"Mills\": row[\"Mills\"],\n",
        "            \"PIOConcessions\": row[\"PIOConcessions\"],\n",
        "            \"Issues\": row[\"Issues\"],\n",
        "            \"Source\": row[\"Source\"],\n",
        "            \"Grievance_List\": row[\"Grievance_List\"],\n",
        "            \"Grievance_Count\": len(row[\"Grievance_List\"])\n",
        "        })\n",
        "        mhid += 1\n",
        "\n",
        "df_final = pd.DataFrame(merged_events)\n",
        "\n",
        "# =====================================================\n",
        "# STEP 4 – ADD GROUP INFO + AIRTABLE GROUPS\n",
        "# =====================================================\n",
        "print(\"\\n[STEP 4] Adding Plot & Mill Groups...\")\n",
        "\n",
        "pio_file = \"Concessions-v2-Grid view (5).csv\"\n",
        "mills_file = \"Mills-Grid view (10).csv\"\n",
        "\n",
        "df_pio = pd.read_csv(pio_file, dtype=str)\n",
        "df_mills = pd.read_csv(mills_file, dtype=str)\n",
        "\n",
        "pio_group = pd.Series(df_pio[\"Group\"].values, index=df_pio[\"ID\"]).to_dict()\n",
        "pio_airtable = pd.Series(df_pio[\"GroupAirtableRecID\"].values, index=df_pio[\"ID\"]).to_dict()\n",
        "\n",
        "mills_group = pd.Series(df_mills[\"Group\"].values, index=df_mills[\"UML_ID\"]).to_dict()\n",
        "mills_airtable = pd.Series(df_mills[\"GroupAirtableRecID\"].values, index=df_mills[\"UML_ID\"]).to_dict()\n",
        "\n",
        "def get_groups(ids_list, mapping_dict):\n",
        "    valid = []\n",
        "    for i in ids_list:\n",
        "        if i in mapping_dict and pd.notna(mapping_dict[i]):\n",
        "            valid.append(str(mapping_dict[i]))\n",
        "    return \", \".join(sorted(set(valid))) if valid else \"\"\n",
        "\n",
        "df_final[\"Plot_Group\"] = df_final[\"PIOConcessions\"].apply(lambda x: get_groups(x, pio_group))\n",
        "df_final[\"Plot_AirtableID_Group\"] = df_final[\"PIOConcessions\"].apply(lambda x: get_groups(x, pio_airtable))\n",
        "\n",
        "df_final[\"Mill_Group\"] = df_final[\"Mills\"].apply(lambda x: get_groups(x, mills_group))\n",
        "df_final[\"Mill_AirtableID_Group\"] = df_final[\"Mills\"].apply(lambda x: get_groups(x, mills_airtable))\n",
        "\n",
        "print(\"✓ Added Plot & Mill group columns\")\n",
        "\n",
        "# =====================================================\n",
        "# STEP 5 – COMPANY TRACKER LOOKUP (FROM ORIGINAL FILE)\n",
        "# =====================================================\n",
        "print(\"\\n[STEP 5] Adding company tracker columns...\")\n",
        "\n",
        "df_lookup = pd.read_csv(INPUT_FILE, dtype=str)\n",
        "df_lookup = df_lookup[[\"ID\", \"Company Tracker\", \"Tracker Company AirtableRecIDs\"]]\n",
        "\n",
        "# Ensure 'ID' column is unique before setting it as index for to_dict('index')\n",
        "df_lookup = df_lookup.drop_duplicates(subset=['ID'], keep='first')\n",
        "\n",
        "tracker_dict = df_lookup.set_index(\"ID\")[[\"Company Tracker\",\"Tracker Company AirtableRecIDs\"]].to_dict(\"index\")\n",
        "\n",
        "def lookup_tracker(grievance_ids):\n",
        "    companies = []\n",
        "    rec_ids = []\n",
        "\n",
        "    for gid in grievance_ids:\n",
        "        if gid in tracker_dict:\n",
        "            comp = tracker_dict[gid][\"Company Tracker\"]\n",
        "            rec = tracker_dict[gid][\"Tracker Company AirtableRecIDs\"]\n",
        "\n",
        "            if pd.notna(comp):\n",
        "                companies.append(comp)\n",
        "            if pd.notna(rec):\n",
        "                rec_ids.append(rec)\n",
        "\n",
        "    return (\n",
        "        \", \".join(sorted(set(companies))),\n",
        "        \", \".join(sorted(set(rec_ids)))\n",
        "    )\n",
        "\n",
        "df_final[[\"Company_Tracker\", \"Tracker_Company_AirtableRecIDs\"]] = \\\n",
        "    df_final[\"Grievance_List\"].apply(lambda x: pd.Series(lookup_tracker(x)))\n",
        "\n",
        "print(\"✓ Added company tracker columns\")\n",
        "\n",
        "# =====================================================\n",
        "# FINAL OUTPUT CLEANING\n",
        "# =====================================================\n",
        "for col in [\"Suppliers\",\"Mills\",\"PIOConcessions\",\"Issues\",\"Source\",\"Grievance_List\"]:\n",
        "    df_final[col] = df_final[col].apply(lambda x: \", \".join(sorted(set(x))))\n",
        "\n",
        "df_final.to_csv(FINAL_OUT, index=False)\n",
        "\n",
        "print(\"\\n✅ FINAL DONE\")\n",
        "print(\"Final MHID count:\", len(df_final))\n",
        "print(\"Output saved to:\", FINAL_OUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whats new\n",
        "\n",
        "1. Split deforestation and Peatland Loss to others issues\n",
        "\n",
        "2. if issues deforestation, then no time window, else use time window = 90"
      ],
      "metadata": {
        "id": "Dnqri7rtS7uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  WHATS NEW\n",
        "  1. fix duplicate MHID"
      ],
      "metadata": {
        "id": "xDUdMR9gxBQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "# =====================================================\n",
        "# CONFIG\n",
        "# =====================================================\n",
        "INPUT_FILE = \"Grievances-Grid view 3.csv\"\n",
        "FINAL_OUT = \"Final_Merged_splitissue_splittime.csv\"\n",
        "TIME_WINDOW_DAYS = 90  # Other issue then deforestation only\n",
        "\n",
        "# =====================================================\n",
        "# HELPERS\n",
        "# =====================================================\n",
        "def split_list(cell):\n",
        "    if pd.isna(cell) or str(cell).strip() == \"\":\n",
        "        return []\n",
        "    s = str(cell).replace(\"[\", \"\").replace(\"]\", \"\")\n",
        "    parts = [p.strip() for p in re.split(\"[,;]\", s)]\n",
        "    return [p for p in parts if p and p.lower() not in (\"nan\",\"none\")]\n",
        "\n",
        "def split_source(val):\n",
        "    if pd.isna(val) or str(val).strip() == \"\":\n",
        "        return []\n",
        "    parts = re.split(r\",(?![\\s])\", str(val))\n",
        "    return [p.strip() for p in parts if p.strip() and p.lower() not in (\"nan\",\"none\")]\n",
        "\n",
        "def normalize_list(lst):\n",
        "    out = []\n",
        "    for v in lst or []:\n",
        "        if pd.isna(v):\n",
        "            continue\n",
        "        s = str(v).strip()\n",
        "        if s == \"\" or s.lower() in (\"nan\",\"none\"):\n",
        "            continue\n",
        "        out.append(s)\n",
        "    return out\n",
        "\n",
        "def uniq_list(x):\n",
        "    return sorted(list(dict.fromkeys(x)))\n",
        "\n",
        "def parse_date_safe(x):\n",
        "    try:\n",
        "        return pd.to_datetime(x, errors=\"coerce\")\n",
        "    except Exception:\n",
        "        return pd.NaT\n",
        "\n",
        "def time_overlap(d1, d2, window_days):\n",
        "    if pd.isna(d1) or pd.isna(d2):\n",
        "        return False\n",
        "    return abs((d1 - d2).days) <= window_days\n",
        "\n",
        "def contains_deforestation_or_peat(issues):\n",
        "    for i in issues or []:\n",
        "        if not i: continue\n",
        "        s = str(i).lower()\n",
        "        if \"deforest\" in s or \"peat\" in s:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# =====================================================\n",
        "# STEP 1 – LOAD + NORMALIZE + EXPAND SOURCE\n",
        "# =====================================================\n",
        "print(\"[STEP 1] Load + Normalize\")\n",
        "\n",
        "df = pd.read_csv(INPUT_FILE, dtype=str)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "df[\"Raw_ID\"] = df.index.astype(int)\n",
        "\n",
        "multi_cols = [\"Suppliers\", \"Mills\", \"PIOConcessions\", \"Issues\"]\n",
        "for col in multi_cols:\n",
        "    df[col] = df[col].apply(split_list)\n",
        "\n",
        "df[\"Source\"] = df[\"Source\"].apply(split_source)\n",
        "\n",
        "expanded = []\n",
        "for _, r in df.iterrows():\n",
        "    sources = r[\"Source\"]\n",
        "    if not sources:\n",
        "        new = r.copy()\n",
        "        new[\"Source\"] = []\n",
        "        expanded.append(new)\n",
        "    else:\n",
        "        for s in sources:\n",
        "            new = r.copy()\n",
        "            new[\"Source\"] = [s]\n",
        "            expanded.append(new)\n",
        "\n",
        "df_expanded = pd.DataFrame(expanded).reset_index(drop=True)\n",
        "print(\"Expanded rows:\", len(df_expanded))\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 2 – MERGE PER SOURCE (initial clustering)\n",
        "# =====================================================\n",
        "print(\"[STEP 2] Merge per Source\")\n",
        "\n",
        "events = []\n",
        "evt_id = 1\n",
        "\n",
        "# group by first source value (or None)\n",
        "group_keys = df_expanded[\"Source\"].apply(lambda x: x[0] if isinstance(x, list) and x else None)\n",
        "for key, grp in df_expanded.groupby(group_keys):\n",
        "    source_events = []\n",
        "    for _, row in grp.iterrows():\n",
        "        sup = set(normalize_list(row.get(\"Suppliers\", [])))\n",
        "        mil = set(normalize_list(row.get(\"Mills\", [])))\n",
        "        pio = set(normalize_list(row.get(\"PIOConcessions\", [])))\n",
        "        iss = normalize_list(row.get(\"Issues\", []))\n",
        "        gid_raw = row.get(\"ID\", \"\")\n",
        "        gid = str(gid_raw).strip() if pd.notna(gid_raw) else \"\"\n",
        "        if gid == \"\":\n",
        "            # fallback: use Raw_ID for traceability\n",
        "            gid = f\"RAW_{row.get('Raw_ID','')}\"\n",
        "        date_filed_raw = row.get(\"Date Filed\", \"\")\n",
        "        date_filed = str(date_filed_raw).strip()\n",
        "\n",
        "        merged = False\n",
        "        for evt in source_events:\n",
        "            evt_sup = set(normalize_list(evt[\"Suppliers\"]))\n",
        "            evt_mil = set(normalize_list(evt[\"Mills\"]))\n",
        "            evt_pio = set(normalize_list(evt[\"PIOConcessions\"]))\n",
        "\n",
        "            overlap = bool(sup & evt_sup) or bool(mil & evt_mil) or bool(pio & evt_pio)\n",
        "            if overlap:\n",
        "                evt[\"Suppliers\"] = uniq_list(evt[\"Suppliers\"] + list(sup))\n",
        "                evt[\"Mills\"] = uniq_list(evt[\"Mills\"] + list(mil))\n",
        "                evt[\"PIOConcessions\"] = uniq_list(evt[\"PIOConcessions\"] + list(pio))\n",
        "                evt[\"Issues\"] = uniq_list(evt[\"Issues\"] + iss)\n",
        "                evt[\"Grievance_List\"].append(gid)\n",
        "                evt[\"Date_Filed_List\"].append(date_filed)\n",
        "                merged = True\n",
        "                break\n",
        "        if not merged:\n",
        "            source_events.append({\n",
        "                \"Event_ID\": f\"EVT_{evt_id}\",\n",
        "                \"Source\": [key] if key is not None else [],\n",
        "                \"Suppliers\": list(sup),\n",
        "                \"Mills\": list(mil),\n",
        "                \"PIOConcessions\": list(pio),\n",
        "                \"Issues\": iss,\n",
        "                \"Grievance_List\": [gid],\n",
        "                \"Date_Filed_List\": [date_filed]\n",
        "            })\n",
        "            evt_id += 1\n",
        "    events.extend(source_events)\n",
        "\n",
        "df_step2 = pd.DataFrame(events)\n",
        "print(\"Step2 events:\", len(df_step2))\n",
        "\n",
        "# =====================================================\n",
        "# STEP 2.5 – SPLIT GROUP A / B berdasarkan Issues\n",
        "# =====================================================\n",
        "df_step2[\"Issues\"] = df_step2[\"Issues\"].apply(lambda x: normalize_list(x))\n",
        "df_step2[\"Issue_Group\"] = df_step2[\"Issues\"].apply(lambda x: \"A\" if contains_deforestation_or_peat(x) else \"B\")\n",
        "\n",
        "groupA = df_step2[df_step2[\"Issue_Group\"] == \"A\"].copy()\n",
        "groupB = df_step2[df_step2[\"Issue_Group\"] == \"B\"].copy()\n",
        "print(\"Group A:\", len(groupA), \"Group B:\", len(groupB))\n",
        "\n",
        "# =====================================================\n",
        "# STEP 3 – FINAL MERGE (different rules per group)\n",
        "# =====================================================\n",
        "def finalize_merge(df_input, use_time_window):\n",
        "    # prepare date fields\n",
        "    def earliest_date(list_dates):\n",
        "        arr = [parse_date_safe(d) for d in (list_dates or []) if str(d).strip()!=\"\"]\n",
        "        arr = [a for a in arr if not pd.isna(a)]\n",
        "        return min(arr) if arr else pd.NaT\n",
        "\n",
        "    rows = df_input.to_dict(\"records\")\n",
        "    # normalize rows in-place\n",
        "    norm_rows = []\n",
        "    for r in rows:\n",
        "        nr = {}\n",
        "        nr[\"Suppliers\"] = normalize_list(r.get(\"Suppliers\", []))\n",
        "        nr[\"Mills\"] = normalize_list(r.get(\"Mills\", []))\n",
        "        nr[\"PIOConcessions\"] = normalize_list(r.get(\"PIOConcessions\", []))\n",
        "        nr[\"Issues\"] = normalize_list(r.get(\"Issues\", []))\n",
        "        # ensure grievance list items are strings trimmed\n",
        "        nr[\"Grievance_List\"] = [str(x).strip() for x in r.get(\"Grievance_List\", []) if str(x).strip() and str(x).strip().lower() not in (\"nan\",\"none\")]\n",
        "        nr[\"Date_Filed_dt\"] = earliest_date(r.get(\"Date_Filed_List\", []))\n",
        "        nr[\"Source\"] = normalize_list(r.get(\"Source\", []))\n",
        "        norm_rows.append(nr)\n",
        "\n",
        "    merged = []\n",
        "    for r in norm_rows:\n",
        "        did_merge = False\n",
        "        for evt in merged:\n",
        "            evt_sup = set(evt[\"Suppliers\"])\n",
        "            evt_mil = set(evt[\"Mills\"])\n",
        "            evt_pio = set(evt[\"PIOConcessions\"])\n",
        "\n",
        "            supplier_overlap = bool(set(r[\"Suppliers\"]) & evt_sup)\n",
        "\n",
        "            row_has_infra = bool(r[\"Mills\"]) or bool(r[\"PIOConcessions\"])\n",
        "            evt_has_infra = bool(evt[\"Mills\"]) or bool(evt[\"PIOConcessions\"])\n",
        "\n",
        "            infra_overlap = bool(set(r[\"Mills\"]) & evt_mil) or bool(set(r[\"PIOConcessions\"]) & evt_pio)\n",
        "\n",
        "            # determine time_ok if needed\n",
        "            time_ok = True\n",
        "            if use_time_window:\n",
        "                evt_latest = evt.get(\"Latest_Date_dt\", pd.NaT)\n",
        "                time_ok = time_overlap(r[\"Date_Filed_dt\"], evt_latest, TIME_WINDOW_DAYS)\n",
        "\n",
        "            # Merge rules:\n",
        "            # - If both have infra => require supplier_overlap AND infra_overlap (and time_ok if required)\n",
        "            # - Else (one/both missing infra) => require supplier_overlap (and time_ok if required)\n",
        "            if not supplier_overlap:\n",
        "                continue\n",
        "\n",
        "            if row_has_infra and evt_has_infra:\n",
        "                if not infra_overlap:\n",
        "                    continue\n",
        "            # else: supplier_overlap is enough\n",
        "\n",
        "            if use_time_window and not time_ok:\n",
        "                continue\n",
        "\n",
        "            # perform merge\n",
        "            evt[\"Suppliers\"] = uniq_list(evt[\"Suppliers\"] + r[\"Suppliers\"])\n",
        "            evt[\"Mills\"] = uniq_list(evt[\"Mills\"] + r[\"Mills\"])\n",
        "            evt[\"PIOConcessions\"] = uniq_list(evt[\"PIOConcessions\"] + r[\"PIOConcessions\"])\n",
        "            evt[\"Issues\"] = uniq_list(evt[\"Issues\"] + r[\"Issues\"])\n",
        "            evt[\"Source\"] = uniq_list((evt.get(\"Source\") or []) + r.get(\"Source\", []))\n",
        "            evt[\"Grievance_List\"] = uniq_list(evt[\"Grievance_List\"] + r[\"Grievance_List\"])\n",
        "            # update dates\n",
        "            all_dates = [d for d in [evt.get(\"Earliest_Date_dt\", pd.NaT), evt.get(\"Latest_Date_dt\", pd.NaT), r.get(\"Date_Filed_dt\")] if not pd.isna(d)]\n",
        "            if all_dates:\n",
        "                evt[\"Earliest_Date_dt\"] = min(all_dates)\n",
        "                evt[\"Latest_Date_dt\"] = max(all_dates)\n",
        "            else:\n",
        "                evt[\"Earliest_Date_dt\"] = pd.NaT\n",
        "                evt[\"Latest_Date_dt\"] = pd.NaT\n",
        "\n",
        "            did_merge = True\n",
        "            break\n",
        "\n",
        "        if not did_merge:\n",
        "            merged.append({\n",
        "                # DO NOT assign MHID here to avoid duplicates across groups\n",
        "                \"Suppliers\": list(r[\"Suppliers\"]),\n",
        "                \"Mills\": list(r[\"Mills\"]),\n",
        "                \"PIOConcessions\": list(r[\"PIOConcessions\"]),\n",
        "                \"Issues\": list(r[\"Issues\"]),\n",
        "                \"Source\": list(r.get(\"Source\", [])),\n",
        "                \"Grievance_List\": list(r[\"Grievance_List\"]),\n",
        "                \"Earliest_Date_dt\": r.get(\"Date_Filed_dt\", pd.NaT),\n",
        "                \"Latest_Date_dt\": r.get(\"Date_Filed_dt\", pd.NaT)\n",
        "            })\n",
        "\n",
        "    return merged\n",
        "\n",
        "merged_A = finalize_merge(groupA, use_time_window=False)  # ignore time\n",
        "merged_B = finalize_merge(groupB, use_time_window=True)   # respect time window\n",
        "\n",
        "# =====================================================\n",
        "# REASSIGN GLOBAL UNIQUE MHID (fix duplicates)\n",
        "# =====================================================\n",
        "all_events = merged_A + merged_B\n",
        "\n",
        "# ensure each event is normalized lists (safety)\n",
        "for e in all_events:\n",
        "    for k in [\"Suppliers\",\"Mills\",\"PIOConcessions\",\"Issues\",\"Source\",\"Grievance_List\"]:\n",
        "        e[k] = uniq_list(normalize_list(e.get(k, [])))\n",
        "\n",
        "# assign unique MHID sequentially\n",
        "for i, e in enumerate(all_events, start=1):\n",
        "    e[\"MHID\"] = f\"MHID_{i:03d}\"  # zero-padded, change padding if you want\n",
        "\n",
        "# convert to dataframe now\n",
        "df_final = pd.DataFrame(all_events)\n",
        "print(\"Final merged events:\", len(df_final))\n",
        "\n",
        "# =====================================================\n",
        "# STEP 4 – GROUP MAPPING + AIRTABLE IDs\n",
        "# =====================================================\n",
        "print(\"[STEP 4] Map groups & airtable IDs\")\n",
        "\n",
        "pio_file = \"Concessions-v2-Grid view (5).csv\"\n",
        "mills_file = \"Mills-Grid view (10).csv\"\n",
        "\n",
        "df_pio = pd.read_csv(pio_file, dtype=str)\n",
        "df_mills = pd.read_csv(mills_file, dtype=str)\n",
        "\n",
        "pio_group = pd.Series(df_pio[\"Group\"].values, index=df_pio[\"ID\"]).to_dict()\n",
        "pio_air = pd.Series(df_pio[\"GroupAirtableRecID\"].values, index=df_pio[\"ID\"]).to_dict()\n",
        "\n",
        "mills_group = pd.Series(df_mills[\"Group\"].values, index=df_mills[\"UML_ID\"]).to_dict()\n",
        "mills_air = pd.Series(df_mills[\"GroupAirtableRecID\"].values, index=df_mills[\"UML_ID\"]).to_dict()\n",
        "\n",
        "def map_group_ids(ids, mapping):\n",
        "    out = []\n",
        "    for i in ids or []:\n",
        "        if not i: continue\n",
        "        key = str(i).strip()\n",
        "        if key in mapping and pd.notna(mapping[key]):\n",
        "            out.append(str(mapping[key]))\n",
        "    return \", \".join(sorted(set(out))) if out else \"\"\n",
        "\n",
        "# map; these expect list inputs, ensure lists exist\n",
        "df_final[\"Plot_Group\"] = df_final[\"PIOConcessions\"].apply(lambda x: map_group_ids(x, pio_group))\n",
        "df_final[\"Plot_AirtableID_Group\"] = df_final[\"PIOConcessions\"].apply(lambda x: map_group_ids(x, pio_air))\n",
        "df_final[\"Mill_Group\"] = df_final[\"Mills\"].apply(lambda x: map_group_ids(x, mills_group))\n",
        "df_final[\"Mill_AirtableID_Group\"] = df_final[\"Mills\"].apply(lambda x: map_group_ids(x, mills_air))\n",
        "\n",
        "# =====================================================\n",
        "# STEP 5 – COMPANY TRACKER LOOKUP\n",
        "# =====================================================\n",
        "print(\"[STEP 5] Company tracker lookup\")\n",
        "\n",
        "tracker_df = pd.read_csv(INPUT_FILE, dtype=str)\n",
        "if \"ID\" in tracker_df.columns:\n",
        "    tracker_df[\"ID\"] = tracker_df[\"ID\"].astype(str).str.strip()\n",
        "    tracker_df = tracker_df.drop_duplicates(subset=['ID'], keep='first').set_index(\"ID\")\n",
        "else:\n",
        "    # create empty tracker df if ID missing\n",
        "    tracker_df = pd.DataFrame(columns=[\"Company Tracker\",\"Tracker Company AirtableRecIDs\"]).set_index(pd.Index([], name=\"ID\"))\n",
        "\n",
        "def lookup_tracker_fields(griev_list):\n",
        "    comps, recids = [], []\n",
        "    for gid in griev_list or []:\n",
        "        g = str(gid).strip()\n",
        "        if g in tracker_df.index:\n",
        "            comp_val = tracker_df.at[g, \"Company Tracker\"] if \"Company Tracker\" in tracker_df.columns else \"\"\n",
        "            rec_val = tracker_df.at[g, \"Tracker Company AirtableRecIDs\"] if \"Tracker Company AirtableRecIDs\" in tracker_df.columns else \"\"\n",
        "            comp_str = str(comp_val).strip() if pd.notna(comp_val) else \"\"\n",
        "            rec_str = str(rec_val).strip() if pd.notna(rec_val) else \"\"\n",
        "            if comp_str:\n",
        "                comps.append(comp_str)\n",
        "            if rec_str:\n",
        "                recids.append(rec_str)\n",
        "    return \", \".join(sorted(set(comps))), \", \".join(sorted(set(recids)))\n",
        "\n",
        "if not df_final.empty:\n",
        "    df_final[[\"Company_Tracker\",\"Tracker_Company_AirtableRecIDs\"]] = df_final[\"Grievance_List\"].apply(lambda x: pd.Series(lookup_tracker_fields(x)))\n",
        "else:\n",
        "    df_final[\"Company_Tracker\"] = \"\"\n",
        "    df_final[\"Tracker_Company_AirtableRecIDs\"] = \"\"\n",
        "\n",
        "# =====================================================\n",
        "# STEP 5.5 – ADD GRIEVANCE COUNT (unique)\n",
        "# =====================================================\n",
        "print(\"[STEP 5.5] Adding grievance count\")\n",
        "def count_grievances(griev_list):\n",
        "    if not isinstance(griev_list, list):\n",
        "        return 0\n",
        "    clean = [str(g).strip() for g in griev_list if str(g).strip() not in (\"\", \"nan\", \"None\")]\n",
        "    return len(set(clean))\n",
        "\n",
        "df_final[\"Grievance_Count\"] = df_final[\"Grievance_List\"].apply(count_grievances)\n",
        "\n",
        "# =====================================================\n",
        "# FINAL CLEAN & EXPORT\n",
        "# =====================================================\n",
        "# turn list columns into comma-separated strings for export\n",
        "list_cols = [\"Suppliers\",\"Mills\",\"PIOConcessions\",\"Issues\",\"Source\",\"Grievance_List\"]\n",
        "for col in list_cols:\n",
        "    if col in df_final.columns:\n",
        "        df_final[col] = df_final[col].apply(lambda x: \", \".join(sorted(set([str(i).strip() for i in x]))) if isinstance(x, list) else (str(x) if pd.notna(x) else \"\"))\n",
        "\n",
        "if \"Earliest_Date_dt\" in df_final.columns:\n",
        "    df_final[\"Earliest_Date\"] = pd.to_datetime(df_final[\"Earliest_Date_dt\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "else:\n",
        "    df_final[\"Earliest_Date\"] = \"\"\n",
        "\n",
        "if \"Latest_Date_dt\" in df_final.columns:\n",
        "    df_final[\"Latest_Date\"] = pd.to_datetime(df_final[\"Latest_Date_dt\"], errors=\"coerce\").dt.strftime(\"%Y-%m-%d\")\n",
        "else:\n",
        "    df_final[\"Latest_Date\"] = \"\"\n",
        "\n",
        "# drop helpers\n",
        "for c in [\"Earliest_Date_dt\",\"Latest_Date_dt\"]:\n",
        "    if c in df_final.columns:\n",
        "        df_final.drop(columns=[c], inplace=True)\n",
        "\n",
        "# ensure MHID first column for readability\n",
        "cols = df_final.columns.tolist()\n",
        "if \"MHID\" in cols:\n",
        "    cols = [\"MHID\"] + [c for c in cols if c != \"MHID\"]\n",
        "    df_final = df_final[cols]\n",
        "\n",
        "df_final.to_csv(FINAL_OUT, index=False)\n",
        "print(\"Saved:\", FINAL_OUT)\n",
        "print(\"Total MHID:\", len(df_final))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_TfJdvSxgtC",
        "outputId": "f38df149-dd5d-4872-d02c-48650fde1a3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[STEP 1] Load + Normalize\n",
            "Expanded rows: 3488\n",
            "[STEP 2] Merge per Source\n",
            "Step2 events: 1376\n",
            "Group A: 1122 Group B: 254\n",
            "Final merged events: 547\n",
            "[STEP 4] Map groups & airtable IDs\n",
            "[STEP 5] Company tracker lookup\n",
            "[STEP 5.5] Adding grievance count\n",
            "Saved: Final_Merged_splitissue_splittime.csv\n",
            "Total MHID: 547\n"
          ]
        }
      ]
    }
  ]
}